{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1918,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1919,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Tokenization of Question\n",
    "def tokenization(list_of_strings):\n",
    "    BOW_Query=list_of_strings.split()\n",
    "    return BOW_Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1920,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Removal of Stop words in the Question\n",
    "def stop_words(strings_list):\n",
    "    \n",
    "    stop = set(stopwords.words('english'))\n",
    "    bow_query_sw=([i for i in strings_list if i not in stop])\n",
    "    return bow_query_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1921,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Stemming of the Question terms\n",
    "def stemmer(strings_list):\n",
    "    \n",
    "    query_words=[]\n",
    "    ps = PorterStemmer()\n",
    "    for w in strings_list:\n",
    "        query_words.append(ps.stem(w))\n",
    "    return query_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1922,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Removal of stop words in the answers\n",
    "def sw_co(comments):    \n",
    "    stop = set(stopwords.words('english'))\n",
    "    comments_sw=[]\n",
    "    for item in comments:\n",
    "        stop_words=([i for i in item.lower().split() if i not in stop])\n",
    "        comments_sw.append(stop_words)\n",
    "    return comments_sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1923,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Stemming the words in the comments\n",
    "def stemmer_co(strings):\n",
    "    comments_st=[]\n",
    "    ps = PorterStemmer()\n",
    "    for items in strings:\n",
    "        comment_st_each=[]\n",
    "        for w in items:\n",
    "            comment_st_each.append(ps.stem(w))\n",
    "        comments_st.append(comment_st_each)\n",
    "    return comments_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1924,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Creating the unique words list from the above list.\n",
    "#Creating a dictionary with word counter\n",
    "def word_counter(strings):    \n",
    "    my_counter = {}\n",
    "    for line in strings:\n",
    "        for word in line:\n",
    "            my_counter[word] = my_counter.get(word, 0) + 1\n",
    "    return my_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1925,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Extracting only the words (which corresponds to the dictionary keys)\n",
    "def unique_words(dictionary):    \n",
    "    comments_words=[]\n",
    "    for item in dictionary.keys():\n",
    "        comments_words.append(item)\n",
    "    return comments_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1926,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def wordset(querydoc,commentsdoc):\n",
    "    wordset=set(querydoc).union(set(commentsdoc))\n",
    "    return wordset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1927,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Initialization with Zeros\n",
    "def tf_raw_query(query_strings,wordset):    \n",
    "    worddict_query=dict.fromkeys(wordset,0)    \n",
    "    for word in query_strings:\n",
    "        worddict_query[word]+=1\n",
    "    return worddict_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1928,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculation of the tf-weight terms\n",
    "def tf_weight(words_raw):    \n",
    "    dict_tf_wt={}\n",
    "    import math\n",
    "    for key,val in words_raw.items():\n",
    "        if val!=0:\n",
    "            value=1+(math.log(val))\n",
    "        else:\n",
    "            value=0\n",
    "        dict_tf_wt[key]=value\n",
    "    return dict_tf_wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1929,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dfdfdf(ww,dd):\n",
    "    dict_dfdf={}\n",
    "    for item in ww:\n",
    "        #print (item)\n",
    "        co=0\n",
    "        for items in Comments_Stemming:\n",
    "            #print (items)\n",
    "            if item in items:\n",
    "                co+=1\n",
    "            else:\n",
    "                pass\n",
    "            dict_dfdf[item]=co\n",
    "            #print ('First word')\n",
    "    return dict_dfdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#Find the documents frequency - Number of documents a term exist\n",
    "def df_df(wordsetx,docs):   \n",
    "    dict_df={}\n",
    "    for item in wordsetx:\n",
    "        dfg=0\n",
    "        for item2 in docs:\n",
    "            if item in item2:\n",
    "                dfg+=1            \n",
    "            else:\n",
    "                dfg=0\n",
    "            dict_df[item]=dfg\n",
    "    return dict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1930,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculating the idf for a query\n",
    "def idf_query1(dfw):\n",
    "    idf=[]\n",
    "    import math\n",
    "    for item in dfw['df']:\n",
    "        if item !=0:\n",
    "            value=math.log(len(Comments_Stemming)/item)\n",
    "        else:\n",
    "            value=0\n",
    "        idf.append(value)\n",
    "    dfw[\"idf\"]=idf\n",
    "    return dfw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1931,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Normalizing the weights\n",
    "def norm_q(df_norm):    \n",
    "    sum1=0.0\n",
    "    for item in df_norm['wt']:\n",
    "        sq=item*item\n",
    "        sum1+=sq\n",
    "    import math\n",
    "    norm_q=[]\n",
    "    for itemm in df_norm['wt']:\n",
    "        if itemm==0:\n",
    "            deno=0\n",
    "        else:\n",
    "            deno=itemm/(math.sqrt((sum1)))\n",
    "        norm_q.append(deno)\n",
    "    df_norm[\"nor_q\"]=norm_q\n",
    "    return df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1932,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def log_wt(val1):\n",
    "    import math\n",
    "    if val1!=0:\n",
    "        value=1+(math.log(val1))\n",
    "    else:\n",
    "        value=0\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the Question-Comments XML file,Identifying the Question-Comments within the XML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1933,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Importing the XML file and required libraries\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "tree = ET.parse('small_train_2.xml')\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1934,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing the Question ID  Q1_R1\n",
      "Accessing the comments for  Q1_R1\n",
      "printing normalized query vector\n",
      "             tf_raw  tf_weight   df       idf        wt     nor_q\n",
      "-               0.0        0.0  2.0  1.609438  0.000000  0.000000\n",
      ";)              0.0        0.0  2.0  1.609438  0.000000  0.000000\n",
      "?               0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "afford          0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "al-nadir.       0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "area.           0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "behind          0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "call            0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "chines          0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "classi          0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "couldn't        0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "don't           0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "dont            0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "filter          0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "find            1.0        1.0  2.0  1.609438  1.609438  0.707107\n",
      "for.            0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "fu              0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "girls;want      0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "go              0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "helpful.        0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "i'am            0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "it'             0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "kahrama         0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "lawa...y        0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "lol             0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "look            0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "massag          1.0        1.0  0.0  0.000000  0.000000  0.000000\n",
      "mean            0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "name            0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "naseem          0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "nation          0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "next            0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "note            0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "offer?          0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "oil             1.0        1.0  2.0  1.609438  1.609438  0.707107\n",
      "place           1.0        1.0  0.0  0.000000  0.000000  0.000000\n",
      "pleas           0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "price           0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "qatar?          1.0        1.0  0.0  0.000000  0.000000  0.000000\n",
      "right           0.0        0.0  3.0  1.203973  0.000000  0.000000\n",
      "salesgirl       0.0        0.0  2.0  1.609438  0.000000  0.000000\n",
      "salesgirls.     0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "scent           1.0        1.0  0.0  0.000000  0.000000  0.000000\n",
      "seriou          0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "shop.           0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "shop?           0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "smartlink       0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "swine           0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "tast            0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "there.          0.0        0.0  2.0  1.609438  0.000000  0.000000\n",
      "tri             0.0        0.0  3.0  1.203973  0.000000  0.000000\n",
      "u               0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "want            0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "what            0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "ye              0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "yes.            0.0        0.0  1.0  2.302585  0.000000  0.000000\n",
      "you'll          0.0        0.0  2.0  1.609438  0.000000  0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jessica\\Miniconda3\\envs\\py35\\lib\\site-packages\\ipykernel\\__main__.py:125: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing the comments weight after normalization\n",
      "                    1        2         3    4         5        6         7  \\\n",
      "-            0.000000  0.00000  0.000000  0.0  0.251048  0.00000  0.000000   \n",
      ";)           0.000000  0.00000  0.000000  0.0  0.251048  0.00000  0.000000   \n",
      "?            0.000000  0.00000  0.000000  0.0  0.000000  0.00000  0.000000   \n",
      "afford       0.000000  0.00000  0.258199  0.0  0.000000  0.00000  0.000000   \n",
      "al-nadir.    0.000000  0.00000  0.258199  0.0  0.000000  0.00000  0.000000   \n",
      "area.        0.408248  0.00000  0.000000  0.0  0.000000  0.00000  0.000000   \n",
      "behind       0.408248  0.00000  0.000000  0.0  0.000000  0.00000  0.000000   \n",
      "call         0.000000  0.00000  0.258199  0.0  0.000000  0.00000  0.000000   \n",
      "chines       0.000000  0.00000  0.258199  0.0  0.000000  0.00000  0.000000   \n",
      "classi       0.000000  0.00000  0.000000  0.0  0.000000  0.00000  0.000000   \n",
      "couldn't     0.000000  0.00000  0.000000  0.0  0.000000  0.00000  0.447214   \n",
      "don't        0.000000  0.00000  0.000000  0.0  0.000000  0.00000  0.000000   \n",
      "dont         0.000000  0.00000  0.000000  0.5  0.000000  0.00000  0.000000   \n",
      "filter       0.000000  0.00000  0.000000  0.0  0.000000  0.57735  0.000000   \n",
      "find         0.000000  0.00000  0.258199  0.0  0.251048  0.00000  0.000000   \n",
      "for.         0.000000  0.00000  0.000000  0.0  0.251048  0.00000  0.000000   \n",
      "fu           0.000000  0.00000  0.000000  0.0  0.000000  0.00000  0.000000   \n",
      "girls;want   0.000000  0.00000  0.000000  0.5  0.000000  0.00000  0.000000   \n",
      "go           0.000000  0.00000  0.000000  0.0  0.251048  0.00000  0.000000   \n",
      "helpful.     0.000000  0.00000  0.000000  0.0  0.251048  0.00000  0.000000   \n",
      "i'am         0.000000  0.00000  0.000000  0.0  0.251048  0.00000  0.000000   \n",
      "it'          0.000000  0.00000  0.258199  0.0  0.000000  0.00000  0.000000   \n",
      "kahrama      0.408248  0.00000  0.000000  0.0  0.000000  0.00000  0.000000   \n",
      "lawa...y     0.000000  0.00000  0.000000  0.0  0.000000  0.00000  0.447214   \n",
      "lol          0.000000  0.00000  0.000000  0.0  0.000000  0.00000  0.447214   \n",
      "look         0.000000  0.00000  0.000000  0.0  0.251048  0.00000  0.000000   \n",
      "massag       0.000000  0.00000  0.000000  0.0  0.000000  0.00000  0.000000   \n",
      "mean         0.000000  0.00000  0.000000  0.0  0.000000  0.57735  0.000000   \n",
      "name         0.000000  0.57735  0.000000  0.0  0.000000  0.00000  0.000000   \n",
      "naseem       0.000000  0.00000  0.258199  0.0  0.000000  0.00000  0.000000   \n",
      "nation       0.408248  0.00000  0.000000  0.0  0.000000  0.00000  0.000000   \n",
      "next         0.000000  0.00000  0.258199  0.0  0.000000  0.00000  0.000000   \n",
      "note         0.000000  0.00000  0.000000  0.0  0.251048  0.00000  0.000000   \n",
      "offer?       0.000000  0.00000  0.000000  0.0  0.000000  0.00000  0.000000   \n",
      "oil          0.000000  0.00000  0.000000  0.5  0.000000  0.57735  0.000000   \n",
      "place        0.000000  0.00000  0.000000  0.0  0.000000  0.00000  0.000000   \n",
      "pleas        0.000000  0.00000  0.000000  0.0  0.251048  0.00000  0.000000   \n",
      "price        0.000000  0.00000  0.258199  0.0  0.000000  0.00000  0.000000   \n",
      "qatar?       0.000000  0.00000  0.000000  0.0  0.000000  0.00000  0.000000   \n",
      "right        0.408248  0.00000  0.258199  0.0  0.000000  0.00000  0.447214   \n",
      "salesgirl    0.000000  0.00000  0.258199  0.0  0.000000  0.00000  0.000000   \n",
      "salesgirls.  0.000000  0.00000  0.000000  0.0  0.000000  0.00000  0.000000   \n",
      "scent        0.000000  0.00000  0.000000  0.0  0.000000  0.00000  0.000000   \n",
      "seriou       0.000000  0.00000  0.000000  0.0  0.251048  0.00000  0.000000   \n",
      "shop.        0.000000  0.00000  0.258199  0.0  0.000000  0.00000  0.000000   \n",
      "shop?        0.000000  0.57735  0.000000  0.0  0.000000  0.00000  0.000000   \n",
      "smartlink    0.000000  0.00000  0.258199  0.0  0.000000  0.00000  0.000000   \n",
      "swine        0.000000  0.00000  0.000000  0.0  0.000000  0.00000  0.000000   \n",
      "tast         0.000000  0.00000  0.000000  0.0  0.000000  0.00000  0.000000   \n",
      "there.       0.000000  0.00000  0.258199  0.0  0.251048  0.00000  0.000000   \n",
      "tri          0.000000  0.00000  0.000000  0.0  0.425061  0.00000  0.000000   \n",
      "u            0.000000  0.00000  0.000000  0.0  0.000000  0.00000  0.000000   \n",
      "want         0.000000  0.00000  0.000000  0.5  0.000000  0.00000  0.000000   \n",
      "what         0.000000  0.57735  0.000000  0.0  0.000000  0.00000  0.000000   \n",
      "ye           0.000000  0.00000  0.000000  0.0  0.000000  0.00000  0.447214   \n",
      "yes.         0.408248  0.00000  0.000000  0.0  0.000000  0.00000  0.000000   \n",
      "you'll       0.000000  0.00000  0.258199  0.0  0.251048  0.00000  0.000000   \n",
      "\n",
      "               8         9        10  \n",
      "-            0.0  0.000000  0.353553  \n",
      ";)           0.0  0.000000  0.353553  \n",
      "?            0.0  0.447214  0.000000  \n",
      "afford       0.0  0.000000  0.000000  \n",
      "al-nadir.    0.0  0.000000  0.000000  \n",
      "area.        0.0  0.000000  0.000000  \n",
      "behind       0.0  0.000000  0.000000  \n",
      "call         0.0  0.000000  0.000000  \n",
      "chines       0.0  0.000000  0.000000  \n",
      "classi       0.0  0.000000  0.353553  \n",
      "couldn't     0.0  0.000000  0.000000  \n",
      "don't        0.0  0.000000  0.353553  \n",
      "dont         0.0  0.000000  0.000000  \n",
      "filter       0.0  0.000000  0.000000  \n",
      "find         0.0  0.000000  0.000000  \n",
      "for.         0.0  0.000000  0.000000  \n",
      "fu           0.0  0.447214  0.000000  \n",
      "girls;want   0.0  0.000000  0.000000  \n",
      "go           0.0  0.000000  0.000000  \n",
      "helpful.     0.0  0.000000  0.000000  \n",
      "i'am         0.0  0.000000  0.000000  \n",
      "it'          0.0  0.000000  0.000000  \n",
      "kahrama      0.0  0.000000  0.000000  \n",
      "lawa...y     0.0  0.000000  0.000000  \n",
      "lol          0.0  0.000000  0.000000  \n",
      "look         0.0  0.000000  0.000000  \n",
      "massag       0.0  0.000000  0.000000  \n",
      "mean         0.0  0.000000  0.000000  \n",
      "name         0.0  0.000000  0.000000  \n",
      "naseem       0.0  0.000000  0.000000  \n",
      "nation       0.0  0.000000  0.000000  \n",
      "next         0.0  0.000000  0.000000  \n",
      "note         0.0  0.000000  0.000000  \n",
      "offer?       1.0  0.000000  0.000000  \n",
      "oil          0.0  0.000000  0.000000  \n",
      "place        0.0  0.000000  0.000000  \n",
      "pleas        0.0  0.000000  0.000000  \n",
      "price        0.0  0.000000  0.000000  \n",
      "qatar?       0.0  0.000000  0.000000  \n",
      "right        0.0  0.000000  0.000000  \n",
      "salesgirl    0.0  0.447214  0.000000  \n",
      "salesgirls.  0.0  0.000000  0.353553  \n",
      "scent        0.0  0.000000  0.000000  \n",
      "seriou       0.0  0.000000  0.000000  \n",
      "shop.        0.0  0.000000  0.000000  \n",
      "shop?        0.0  0.000000  0.000000  \n",
      "smartlink    0.0  0.000000  0.000000  \n",
      "swine        0.0  0.000000  0.353553  \n",
      "tast         0.0  0.000000  0.353553  \n",
      "there.       0.0  0.000000  0.000000  \n",
      "tri          0.0  0.447214  0.353553  \n",
      "u            0.0  0.447214  0.000000  \n",
      "want         0.0  0.000000  0.000000  \n",
      "what         0.0  0.000000  0.000000  \n",
      "ye           0.0  0.000000  0.000000  \n",
      "yes.         0.0  0.000000  0.000000  \n",
      "you'll       0.0  0.000000  0.000000  \n",
      "{'Q1_R1_C4': 0.35355339059327379, 'Q1_R1_C2': 0.0, 'Q1_R1_C3': 0.18257418583505536, 'Q1_R1_C1': 0.0, 'Q1_R1_C10': 0.0, 'Q1_R1_C8': 0.0, 'Q1_R1_C7': 0.0, 'Q1_R1_C5': 0.17751744913911857, 'Q1_R1_C9': 0.0, 'Q1_R1_C6': 0.40824829046386307}\n",
      "printing cosine score\n",
      "{'Q1_R1_C4': 0.35355339059327379, 'Q1_R1_C2': 0.0, 'Q1_R1_C3': 0.18257418583505536, 'Q1_R1_C1': 0.0, 'Q1_R1_C10': 0.0, 'Q1_R1_C8': 0.0, 'Q1_R1_C7': 0.0, 'Q1_R1_C5': 0.17751744913911857, 'Q1_R1_C9': 0.0, 'Q1_R1_C6': 0.40824829046386307}\n",
      "Accessing the Question ID  Q1_R6\n",
      "Accessing the comments for  Q1_R6\n",
      "printing normalized query vector\n",
      "                       tf_raw  tf_weight   df       idf   wt  nor_q\n",
      "\"\"valu                    0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "\"the                      0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "(besid                    0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "(mega                     0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "01                        1.0        1.0  0.0  0.000000  0.0    0.0\n",
      "150                       0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "300qr                     1.0        1.0  0.0  0.000000  0.0    0.0\n",
      "44274037.                 0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "44410410.                 0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "60qr                      0.0        0.0  1.0  2.302585  0.0    0.0\n",
      ":)                        0.0        0.0  1.0  2.302585  0.0    0.0\n",
      ";)                        0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "=(                        0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "@                         0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "abu                       0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "advic                     1.0        1.0  0.0  0.000000  0.0    0.0\n",
      "anybodi                   0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "aromatherapi              0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "bali                      0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "beauti                    0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "best                      0.0        0.0  2.0  1.609438  0.0    0.0\n",
      "bio-bil                   1.0        1.0  0.0  0.000000  0.0    0.0\n",
      "blue                      0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "bodi                      0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "bt                        1.0        1.0  0.0  0.000000  0.0    0.0\n",
      "build                     0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "call                      0.0        0.0  3.0  1.203973  0.0    0.0\n",
      "center                    0.0        0.0  2.0  1.609438  0.0    0.0\n",
      "charg                     1.0        1.0  0.0  0.000000  0.0    0.0\n",
      "color                     0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "...                       ...        ...  ...       ...  ...    ...\n",
      "roundabout                0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "roy;                      0.0        0.0  2.0  1.609438  0.0    0.0\n",
      "rub                       0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "salon                     0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "seen                      0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "service.                  0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "sooooo                    0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "station                   0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "stn)it                    0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "tell                      1.0        1.0  0.0  0.000000  0.0    0.0\n",
      "thank                     0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "thanks;                   0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "them.                     0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "therapist                 0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "there                     1.0        1.0  0.0  0.000000  0.0    0.0\n",
      "time                      0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "tissu                     0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "total                     1.0        1.0  0.0  0.000000  0.0    0.0\n",
      "touch                     0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "tri                       0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "vacation;                 0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "wast                      0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "waste...                  1.0        1.0  0.0  0.000000  0.0    0.0\n",
      "what?                     0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "window);                  0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "woqood                    0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "work                      0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "would                     0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "www.merzamhotels.com\"     0.0        0.0  1.0  2.302585  0.0    0.0\n",
      "yesterday                 1.0        1.0  0.0  0.000000  0.0    0.0\n",
      "\n",
      "[140 rows x 6 columns]\n",
      "printing the comments weight after normalization\n",
      "                              1         2         3         4         5  \\\n",
      "\"\"valu                 0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "\"the                   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "(besid                 0.000000  0.000000  0.000000  0.219616  0.000000   \n",
      "(mega                  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "01                     0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "150                    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "300qr                  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "44274037.              0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "44410410.              0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "60qr                   0.000000  0.000000  0.000000  0.219616  0.000000   \n",
      ":)                     0.000000  0.000000  0.000000  0.000000  0.218914   \n",
      ";)                     0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "=(                     0.000000  0.258199  0.000000  0.000000  0.000000   \n",
      "@                      0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "abu                    0.000000  0.000000  0.000000  0.371842  0.000000   \n",
      "advic                  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "anybodi                0.000000  0.000000  0.000000  0.000000  0.218914   \n",
      "aromatherapi           0.000000  0.258199  0.000000  0.000000  0.000000   \n",
      "bali                   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "beauti                 0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "best                   0.000000  0.258199  0.000000  0.000000  0.000000   \n",
      "bio-bil                0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "blue                   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "bodi                   0.290291  0.000000  0.000000  0.000000  0.000000   \n",
      "bt                     0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "build                  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "call                   0.000000  0.258199  0.000000  0.000000  0.000000   \n",
      "center                 0.000000  0.000000  0.316228  0.000000  0.000000   \n",
      "charg                  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "color                  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "...                         ...       ...       ...       ...       ...   \n",
      "roundabout             0.000000  0.000000  0.316228  0.000000  0.000000   \n",
      "roy;                   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "rub                    0.290291  0.000000  0.000000  0.000000  0.000000   \n",
      "salon                  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "seen                   0.000000  0.000000  0.000000  0.219616  0.000000   \n",
      "service.               0.000000  0.258199  0.000000  0.000000  0.000000   \n",
      "sooooo                 0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "station                0.000000  0.000000  0.316228  0.000000  0.000000   \n",
      "stn)it                 0.000000  0.000000  0.000000  0.219616  0.000000   \n",
      "tell                   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "thank                  0.000000  0.000000  0.000000  0.000000  0.218914   \n",
      "thanks;                0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "them.                  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "therapist              0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "there                  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "time                   0.000000  0.258199  0.000000  0.000000  0.000000   \n",
      "tissu                  0.290291  0.000000  0.000000  0.000000  0.000000   \n",
      "total                  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "touch                  0.000000  0.000000  0.000000  0.219616  0.000000   \n",
      "tri                    0.000000  0.000000  0.000000  0.219616  0.000000   \n",
      "vacation;              0.000000  0.258199  0.000000  0.000000  0.000000   \n",
      "wast                   0.290291  0.000000  0.000000  0.000000  0.000000   \n",
      "waste...               0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "what?                  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "window);               0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "woqood                 0.000000  0.000000  0.316228  0.000000  0.000000   \n",
      "work                   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "would                  0.000000  0.000000  0.000000  0.000000  0.218914   \n",
      "www.merzamhotels.com\"  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "yesterday              0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "                             6         7         8         9        10  \n",
      "\"\"valu                 0.00000  0.000000  0.000000  0.000000  0.162221  \n",
      "\"the                   0.00000  0.000000  0.000000  0.000000  0.162221  \n",
      "(besid                 0.00000  0.000000  0.000000  0.000000  0.000000  \n",
      "(mega                  0.00000  0.000000  0.000000  0.000000  0.162221  \n",
      "01                     0.00000  0.000000  0.000000  0.000000  0.000000  \n",
      "150                    0.00000  0.000000  0.000000  0.000000  0.162221  \n",
      "300qr                  0.00000  0.000000  0.000000  0.000000  0.000000  \n",
      "44274037.              0.00000  0.000000  0.000000  0.000000  0.162221  \n",
      "44410410.              0.57735  0.000000  0.000000  0.000000  0.000000  \n",
      "60qr                   0.00000  0.000000  0.000000  0.000000  0.000000  \n",
      ":)                     0.00000  0.000000  0.000000  0.000000  0.000000  \n",
      ";)                     0.00000  0.000000  0.316228  0.000000  0.000000  \n",
      "=(                     0.00000  0.000000  0.000000  0.000000  0.000000  \n",
      "@                      0.00000  0.000000  0.000000  0.000000  0.162221  \n",
      "abu                    0.00000  0.000000  0.000000  0.000000  0.000000  \n",
      "advic                  0.00000  0.000000  0.000000  0.000000  0.000000  \n",
      "anybodi                0.00000  0.000000  0.000000  0.000000  0.000000  \n",
      "aromatherapi           0.00000  0.000000  0.000000  0.000000  0.000000  \n",
      "bali                   0.00000  0.000000  0.000000  0.000000  0.162221  \n",
      "beauti                 0.00000  0.000000  0.000000  0.353553  0.000000  \n",
      "best                   0.00000  0.000000  0.000000  0.000000  0.162221  \n",
      "bio-bil                0.00000  0.000000  0.000000  0.000000  0.000000  \n",
      "blue                   0.00000  0.000000  0.000000  0.000000  0.162221  \n",
      "bodi                   0.00000  0.000000  0.000000  0.000000  0.000000  \n",
      "bt                     0.00000  0.000000  0.000000  0.000000  0.000000  \n",
      "build                  0.00000  0.000000  0.000000  0.000000  0.162221  \n",
      "call                   0.57735  0.000000  0.316228  0.000000  0.000000  \n",
      "center                 0.00000  0.000000  0.000000  0.000000  0.162221  \n",
      "charg                  0.00000  0.000000  0.000000  0.000000  0.000000  \n",
      "color                  0.00000  0.000000  0.000000  0.000000  0.162221  \n",
      "...                        ...       ...       ...       ...       ...  \n",
      "roundabout             0.00000  0.000000  0.000000  0.000000  0.000000  \n",
      "roy;                   0.00000  0.408248  0.000000  0.353553  0.000000  \n",
      "rub                    0.00000  0.000000  0.000000  0.000000  0.000000  \n",
      "salon                  0.00000  0.000000  0.000000  0.353553  0.000000  \n",
      "seen                   0.00000  0.000000  0.000000  0.000000  0.000000  \n",
      "service.               0.00000  0.000000  0.000000  0.000000  0.000000  \n",
      "sooooo                 0.00000  0.000000  0.316228  0.000000  0.000000  \n",
      "station                0.00000  0.000000  0.000000  0.000000  0.000000  \n",
      "stn)it                 0.00000  0.000000  0.000000  0.000000  0.000000  \n",
      "tell                   0.00000  0.000000  0.000000  0.000000  0.000000  \n",
      "thank                  0.00000  0.000000  0.000000  0.000000  0.000000  \n",
      "thanks;                0.00000  0.000000  0.000000  0.353553  0.000000  \n",
      "them.                  0.00000  0.000000  0.316228  0.000000  0.000000  \n",
      "therapist              0.00000  0.000000  0.000000  0.000000  0.162221  \n",
      "there                  0.00000  0.000000  0.000000  0.000000  0.000000  \n",
      "time                   0.00000  0.000000  0.000000  0.000000  0.000000  \n",
      "tissu                  0.00000  0.000000  0.000000  0.000000  0.000000  \n",
      "total                  0.00000  0.000000  0.000000  0.000000  0.000000  \n",
      "touch                  0.00000  0.000000  0.000000  0.000000  0.000000  \n",
      "tri                    0.00000  0.000000  0.000000  0.000000  0.000000  \n",
      "vacation;              0.00000  0.000000  0.000000  0.000000  0.000000  \n",
      "wast                   0.00000  0.000000  0.000000  0.000000  0.000000  \n",
      "waste...               0.00000  0.000000  0.000000  0.000000  0.000000  \n",
      "what?                  0.00000  0.000000  0.000000  0.353553  0.000000  \n",
      "window);               0.00000  0.000000  0.000000  0.000000  0.162221  \n",
      "woqood                 0.00000  0.000000  0.000000  0.000000  0.000000  \n",
      "work                   0.00000  0.000000  0.000000  0.000000  0.162221  \n",
      "would                  0.00000  0.000000  0.000000  0.000000  0.000000  \n",
      "www.merzamhotels.com\"  0.00000  0.000000  0.000000  0.000000  0.162221  \n",
      "yesterday              0.00000  0.000000  0.000000  0.000000  0.000000  \n",
      "\n",
      "[140 rows x 10 columns]\n",
      "{'Q1_R6_C4': 0.11856267090339881, 'Q1_R6_C2': 0.0, 'Q1_R6_C10': 0.14658464223962764, 'Q1_R6_C8': 0.0, 'Q1_R6_C1': 0.17878266629643477, 'Q1_R6_C3': 0.11502614012523373, 'Q1_R6_C6': 0.21786232859981472, 'Q1_R6_C9': 0.0, 'Q1_R6_C5': 0.24484206025056465, 'Q1_R6_C7': 0.15405192991802091}\n",
      "printing cosine score\n",
      "{'Q1_R6_C4': 0.11856267090339881, 'Q1_R6_C2': 0.0, 'Q1_R6_C10': 0.14658464223962764, 'Q1_R6_C8': 0.0, 'Q1_R6_C1': 0.17878266629643477, 'Q1_R6_C3': 0.11502614012523373, 'Q1_R6_C6': 0.21786232859981472, 'Q1_R6_C9': 0.0, 'Q1_R6_C5': 0.24484206025056465, 'Q1_R6_C7': 0.15405192991802091}\n",
      "{'Q1_R6_C4': 0.11856267090339881, 'Q1_R1_C8': 0.0, 'Q1_R6_C3': 0.11502614012523373, 'Q1_R6_C9': 0.0, 'Q1_R6_C2': 0.0, 'Q1_R1_C10': 0.0, 'Q1_R1_C7': 0.0, 'Q1_R1_C9': 0.0, 'Q1_R6_C5': 0.24484206025056465, 'Q1_R1_C6': 0.40824829046386307, 'Q1_R1_C4': 0.35355339059327379, 'Q1_R1_C2': 0.0, 'Q1_R1_C3': 0.18257418583505536, 'Q1_R1_C1': 0.0, 'Q1_R6_C7': 0.15405192991802091, 'Q1_R6_C1': 0.17878266629643477, 'Q1_R1_C5': 0.17751744913911857, 'Q1_R6_C10': 0.14658464223962764, 'Q1_R6_C6': 0.21786232859981472, 'Q1_R6_C8': 0.0}\n"
     ]
    }
   ],
   "source": [
    "scores={}\n",
    "for thread in root.findall(\"Thread\"):\n",
    "    dict_score={}\n",
    "    for question in thread.findall('RelQuestion'):\n",
    "        Question_ID=question.attrib['RELQ_ID']\n",
    "        Question_text=question.find('RelQBody').text\n",
    "        \n",
    "        print (\"Accessing the Question ID \",Question_ID)\n",
    "        #Tokenization of the Question        \n",
    "        Tokenized_Question=tokenization(Question_text)\n",
    "        \n",
    "        #Removing the stop words from the Question        \n",
    "        Question_Stop_words=stop_words(Tokenized_Question)        \n",
    "        \n",
    "        #Stemming the words in Question        \n",
    "        Stemmed_question=stemmer(Question_Stop_words)\n",
    "        \n",
    "    print (\"Accessing the comments for \",Question_ID)  \n",
    "    \n",
    "    #Comments\n",
    "    comments=[]\n",
    "    \n",
    "    ids=[]\n",
    "    for each_answer in thread.findall('RelComment'):\n",
    "        Answer_ID=each_answer.attrib['RELC_ID']\n",
    "        Answer_text=each_answer.find('RelCText').text\n",
    "        # Creating the object for LDA model using gensim library\n",
    "        #Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "        # Running and Trainign LDA model on the document term matrix.\n",
    "        #ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "        comments.append(Answer_text)\n",
    "        ids.append(Answer_ID)\n",
    "    \n",
    "       \n",
    "    #Removing stop words from the Comments    \n",
    "    Comments_stop_words=sw_co(comments)\n",
    "    \n",
    "    #Stemming the words in the comments\n",
    "    \n",
    "    Comments_Stemming=stemmer_co(Comments_stop_words) \n",
    "    \n",
    "    \n",
    "    #Creating a wordset for the comments\n",
    "    counter1=word_counter(Comments_Stemming)\n",
    "    unique_words1=unique_words(counter1)\n",
    "    wordset1=wordset(Stemmed_question,unique_words1)\n",
    "    #print (\"printing wordset\")\n",
    "    \n",
    "    \n",
    "    #Calculating the raw terms in the comments\n",
    "    #print ('printing stemmed question')\n",
    "    #print (Stemmed_question)\n",
    "    tf_raw=tf_raw_query(Stemmed_question,wordset1) \n",
    "    #print (tf_raw)\n",
    "    tf_wt=tf_weight(tf_raw)\n",
    "    #print (tf_wt)\n",
    "    #print ('printing wordset1')\n",
    "    dl=list(wordset1)\n",
    "    #print (dl)\n",
    "    \n",
    "    #print ('prining comments stemming')\n",
    "    #print (Comments_Stemming)\n",
    "    #print ('printing df_query')\n",
    "    #print ('Calculating df')\n",
    "    df_query=dfdfdf(dl,Comments_Stemming)\n",
    "    #print ('printing df_query')\n",
    "    #print (df_query)\n",
    "    \n",
    "    #Appending the tf-raw, tf-wt, df into a dataframe\n",
    "    \n",
    "    dframe=pd.DataFrame.from_dict([tf_raw,tf_wt,df_query],orient='columns')\n",
    "    df=dframe.transpose()\n",
    "    #print (df)\n",
    "    summary_of_results=df.rename(columns={0: 'tf_raw',1: 'tf_weight',2: 'df'})\n",
    "    #print (summary_of_results)\n",
    "    #Calculating the IDF for the query\n",
    "    \n",
    "    idf_query=idf_query1(summary_of_results)    \n",
    "    idf_query[\"wt\"]=idf_query.idf*idf_query.tf_weight\n",
    "    #print (idf_query)\n",
    "    #Normalization\n",
    "    \n",
    "    norm_query=norm_q(idf_query)\n",
    "    print ('printing normalized query vector')\n",
    "    print (norm_query)\n",
    "    #Calculating the raw-terms for all the comments\n",
    "    \n",
    "    result=pd.DataFrame()\n",
    "    for item in Comments_Stemming:\n",
    "        worddict_terms=dict.fromkeys(wordset1,0)  \n",
    "        frames=pd.DataFrame()\n",
    "        for items in item:\n",
    "            worddict_terms[items]+=1\n",
    "            df_com_c1=pd.DataFrame.from_dict([worddict_terms])\n",
    "        frames=[result,df_com_c1]\n",
    "        result = pd.concat(frames)\n",
    "    Comments_raw_terms=result.transpose()\n",
    "    #print ('printing comments_raw_terms')\n",
    "    #print (Comments_raw_terms)\n",
    "    #Calculating the terms weight\n",
    "    #print ('printing comments_weights')\n",
    "    Comments_weights=Comments_raw_terms.applymap(log_wt)\n",
    "    \n",
    "    #print (Comments_weights)\n",
    "    #Adding the column name to the Dataframe\n",
    "    Comments_weights.columns=[\"tf_doc_wt_1\",\"tf_doc_wt_2\",\"tf_doc_wt_3\",\"tf_doc_wt_4\",\"tf_doc_wt_5\",\n",
    "                              \"tf_doc_wt_6\",\"tf_doc_wt_7\",\"tf_doc_wt_8\",\"tf_doc_wt_9\",\"tf_doc_wt_10\"]\n",
    "    #print (Comments_weights)\n",
    "\n",
    "    #Normalization of all the comments weights\n",
    "    \n",
    "    counter=1\n",
    "    for item in Comments_weights:\n",
    "        sum2=0.0\n",
    "        for items in Comments_weights[item]:\n",
    "            sq1=items*items\n",
    "            sum2+=sq1\n",
    "            import math\n",
    "            norm_q_d=[]\n",
    "            for itemw in Comments_weights[item]:\n",
    "                if itemw==0:\n",
    "                    deno=0\n",
    "                else:\n",
    "                    deno=itemw/math.sqrt((sum2))\n",
    "                norm_q_d.append(deno)\n",
    "        s=str(counter)\n",
    "        Comments_weights[s]=norm_q_d\n",
    "        counter+=1\n",
    "    #Comments_weights\n",
    "    #Keeping the column weights \n",
    "    cols = [0,1,2,3,4,5,6,7,8,9]\n",
    "    Comments_weights.drop(Comments_weights.columns[cols],axis=1,inplace=True)\n",
    "    print ('printing the comments weight after normalization')\n",
    "    print (Comments_weights)\n",
    "    \n",
    "    #product\n",
    "    #print (\"multipliying the normalized weights\")\n",
    "    columns = Comments_weights.columns[:]\n",
    "    #print (columns)\n",
    "    #print ('start')\n",
    "    for item in columns:\n",
    "        #print (item)\n",
    "        #if item !=0:\n",
    "        Comments_weights[item] *= norm_query['nor_q']\n",
    "        #dict_score[Answer_ID]=Comments_weights.sum(axis=0)\n",
    "        #print (dict_score)\n",
    "        #else:\n",
    "            #pass\n",
    "    #print ('printing normalized query vectors')\n",
    "    #print (norm_query)\n",
    "    #print ('printing product weight comments')\n",
    "    #print (Comments_weights)\n",
    "    #Replacing the NaN values with Zero's\n",
    "    Comments_weights_corrected=Comments_weights.fillna('0')\n",
    "\n",
    "    #Query - Documents score\n",
    "    \n",
    "    Cosine_Score=Comments_weights_corrected.sum(axis=0)\n",
    "    scoreee=list(Cosine_Score)\n",
    "    dictionary = dict(zip(ids, scoreee))\n",
    "    scores.update(dictionary)\n",
    "    print(dictionary)\n",
    "\n",
    "    print ('printing cosine score')\n",
    "    print(dictionary)\n",
    "print (scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1935,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Q1_R6_C1',\n",
       " 'Q1_R6_C2',\n",
       " 'Q1_R6_C3',\n",
       " 'Q1_R6_C4',\n",
       " 'Q1_R6_C5',\n",
       " 'Q1_R6_C6',\n",
       " 'Q1_R6_C7',\n",
       " 'Q1_R6_C8',\n",
       " 'Q1_R6_C9',\n",
       " 'Q1_R6_C10']"
      ]
     },
     "execution_count": 1935,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
